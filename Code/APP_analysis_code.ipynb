{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f21018a5",
   "metadata": {},
   "source": [
    "# Exploring American Presidency Project Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e34e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shime\\Downloads\n",
      "C:\\Users\\shime\\Downloads\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "\n",
    "os.chdir(r\"C:\\Users\\shime\\Downloads\")\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73661091",
   "metadata": {},
   "source": [
    "### We scraped the data using the following scraper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebdcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_URL = \"https://www.presidency.ucsb.edu\"\n",
    "LIST_PATH = \"/documents/app-categories/statements\"\n",
    "LIST_URL = BASE_URL + LIST_PATH\n",
    "OUTFILE = \"presidential_statements.csv\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; Scraper/1.0)\"}\n",
    "TEMP_SAVE_EVERY = 100   # flush every N records\n",
    "DELAY_BETWEEN_REQUESTS = 0.35\n",
    "\n",
    "def fetch(url, timeout=15, tries=3):\n",
    "    for attempt in range(tries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "            resp.raise_for_status()\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            print(f\"Fetch error ({attempt+1}/{tries}) for {url}: {e}\")\n",
    "            time.sleep(1)\n",
    "    return None\n",
    "\n",
    "def extract_detail_content(detail_url):\n",
    "    r = fetch(detail_url)\n",
    "    if not r:\n",
    "        return \"\", \"\", \"\"\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    content_node = s.select_one(\"div.field-docs-content\")\n",
    "    content = content_node.get_text(\"\\n\", strip=True) if content_node else \"\"\n",
    "\n",
    "    # categories (if present)\n",
    "    cats = s.select(\"div.group-meta a, div.field-ds-filed-under- a, .field-ds-filed-under a\")\n",
    "    categories = \", \".join([c.get_text(strip=True) for c in cats]) if cats else \"\"\n",
    "\n",
    "    # citation\n",
    "    cit = s.select_one(\".field-prez-document-citation, .ucsbapp_citation\")\n",
    "    citation = cit.get_text(\" \", strip=True) if cit else \"\"\n",
    "\n",
    "    return content, categories, citation\n",
    "\n",
    "def read_existing_urls(outfile):\n",
    "    p = Path(outfile)\n",
    "    if not p.exists():\n",
    "        return set()\n",
    "    try:\n",
    "        with p.open(\"r\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            return {row.get(\"url\",\"\").strip() for row in reader if row.get(\"url\")}\n",
    "    except Exception as e:\n",
    "        print(\"Error reading existing CSV, will start fresh:\", e)\n",
    "        return set()\n",
    "\n",
    "def scrape_statements(max_pages=None):\n",
    "    existing_urls = read_existing_urls(OUTFILE)\n",
    "    page = 0\n",
    "    total_saved = 0\n",
    "\n",
    "    # prepare CSV writer (append mode)\n",
    "    headers = [\"title\", \"url\", \"president\", \"date\", \"content\", \"categories\", \"citation\"]\n",
    "    outfile_path = Path(OUTFILE)\n",
    "    write_header = not outfile_path.exists()\n",
    "\n",
    "    csvfile = open(outfile_path, \"a\", newline=\"\", encoding=\"utf-8\")\n",
    "    writer = csv.DictWriter(csvfile, \n",
    "                            fieldnames=headers,\n",
    "                            quoting = csv.QUOTE_ALL,\n",
    "                            escapechar='\\\\')\n",
    "    if write_header:\n",
    "        writer.writeheader()\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            if max_pages is not None and page >= max_pages:\n",
    "                print(\"Reached max_pages limit, stopping.\")\n",
    "                break\n",
    "\n",
    "            page_url = f\"{LIST_URL}?page={page}\"\n",
    "            resp = fetch(page_url)\n",
    "            if not resp:\n",
    "                print(\"Failed to fetch listing page:\", page_url)\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "            # listing item containers for statements\n",
    "            items = soup.select(\"div.views-row, div.node-teaser, div.node-documents.node-teaser\")\n",
    "            # filter duplicates and ensure items have a link\n",
    "            items = [it for it in items if it.select_one(\"a[href*='/documents/']\")]\n",
    "\n",
    "            if not items:\n",
    "                print(f\"No items found on page {page}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            num_items = len(items)\n",
    "            print(f\"Scraping page {page+1}: {num_items} statements (Total so far: {total_saved + num_items})\")\n",
    "\n",
    "            for item in items:\n",
    "                # Title + link\n",
    "                title_a = item.select_one(\".field-title a, h3 a, a[href*='/documents/']\")\n",
    "                if not title_a:\n",
    "                    print(\"Skipping item (no title link). snippet:\", item.get_text(\" \", strip=True)[:150])\n",
    "                    continue\n",
    "                title = title_a.get_text(strip=True)\n",
    "                href = title_a.get(\"href\", \"\").strip()\n",
    "                full_link = href if href.startswith(\"http\") else BASE_URL + href\n",
    "\n",
    "                # skip if already scraped\n",
    "                if full_link in existing_urls:\n",
    "                    # print(\"Skipping already-saved:\", full_link)\n",
    "                    continue\n",
    "\n",
    "                # president (the \"Related\" link on the right column)\n",
    "                pres_a = item.select_one(\".col-sm-4 a, .views-field-field-president a, .field-title ~ .col-sm-4 a\")\n",
    "                president = pres_a.get_text(strip=True) if pres_a else \"\"\n",
    "\n",
    "                # date\n",
    "                date_span = item.select_one(\"span.date-display-single, .views-field-field-docs-date span, .views-field-created span\")\n",
    "                date = date_span.get(\"content\", date_span.get_text(strip=True)) if date_span else \"\"\n",
    "\n",
    "                # fetch detail content\n",
    "                content, categories, citation = extract_detail_content(full_link)\n",
    "\n",
    "                row = {\n",
    "                    \"title\": title,\n",
    "                    \"url\": full_link,\n",
    "                    \"president\": president,\n",
    "                    \"date\": date,\n",
    "                    \"content\": content,\n",
    "                    \"categories\": categories,\n",
    "                    \"citation\": citation\n",
    "                }\n",
    "\n",
    "                writer.writerow(row)\n",
    "                existing_urls.add(full_link)\n",
    "                total_saved += 1\n",
    "\n",
    "                if total_saved % TEMP_SAVE_EVERY == 0:\n",
    "                    csvfile.flush()\n",
    "                    print(f\"Checkpoint: saved {total_saved} records so far.\")\n",
    "\n",
    "                time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "            page += 1\n",
    "            # small delay between pages\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    finally:\n",
    "        csvfile.close()\n",
    "\n",
    "    print(\"Finished. Total new records saved:\", total_saved)\n",
    "    return total_saved\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # For a quick test set max_pages=2\n",
    "    # For full run use max_pages=None\n",
    "    scrape_statements(max_pages=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original scraper had issues with formatting columns, so we reformat here\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_final = pd.read_csv(\"presidential_statements.csv\", header=None)\n",
    "\n",
    "df_final.columns = [\n",
    "    \"title\",\n",
    "    \"url\",\n",
    "    \"president\",\n",
    "    \"date\",\n",
    "    \"content\",\n",
    "    \"categories\",\n",
    "    \"citation\"\n",
    "]\n",
    "\n",
    "df_final.to_csv(\"presidential_statements_scraped.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e53f9",
   "metadata": {},
   "source": [
    "# From here below I'm working on the American Presidency presidential statements csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abecba60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12399, 7)\n",
      "                                               title  \\\n",
      "0            Joint Statement on U.S.â€“Ukraine Meeting   \n",
      "1  Statement on Signing the Epstein Files Transpa...   \n",
      "2  Joint Statement on a Framework for a United St...   \n",
      "3  Joint Statement on a Framework for United Stat...   \n",
      "4  Joint Statement on a Framework for United Stat...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  https://www.presidency.ucsb.edu/documents/join...   \n",
      "1  https://www.presidency.ucsb.edu/documents/stat...   \n",
      "2  https://www.presidency.ucsb.edu/documents/join...   \n",
      "3  https://www.presidency.ucsb.edu/documents/join...   \n",
      "4  https://www.presidency.ucsb.edu/documents/join...   \n",
      "\n",
      "                    president                       date  \\\n",
      "0  Donald J. Trump (2nd Term)  2025-11-23T00:00:00+00:00   \n",
      "1  Donald J. Trump (2nd Term)  2025-11-19T00:00:00+00:00   \n",
      "2  Donald J. Trump (2nd Term)  2025-11-14T00:00:00+00:00   \n",
      "3  Donald J. Trump (2nd Term)  2025-11-13T00:00:00+00:00   \n",
      "4  Donald J. Trump (2nd Term)  2025-11-13T00:00:00+00:00   \n",
      "\n",
      "                                             content  \\\n",
      "0  On 23 November 2025, representatives of the Un...   \n",
      "1  Jeffrey Epstein, who was charged by the Trump ...   \n",
      "2  Today, the United States of America (United St...   \n",
      "3  The United States of America (United States, o...   \n",
      "4  President Donald J. Trump and President Daniel...   \n",
      "\n",
      "                                          categories  \\\n",
      "0                    Presidential, Statements, Joint   \n",
      "1  Presidential, Statements, Signing Statements, ...   \n",
      "2                    Presidential, Statements, Joint   \n",
      "3                    Presidential, Statements, Joint   \n",
      "4                    Presidential, Statements, Joint   \n",
      "\n",
      "                                            citation  \n",
      "0  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "1  Donald J. Trump (2nd Term), Statement on Signi...  \n",
      "2  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "3  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "4  Donald J. Trump (2nd Term), Joint Statement on...  \n",
      "Index(['title', 'url', 'president', 'date', 'content', 'categories',\n",
      "       'citation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Read in the CSV file\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"presidential_statements_scraped.csv\")\n",
    "\n",
    "#Inspect the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "#Print the first few rows\n",
    "print(dataset.head())\n",
    "\n",
    "#Check columns\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d1123b",
   "metadata": {},
   "source": [
    "### Adding features for future visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ef09e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['title', 'url', 'president', 'date', 'content', 'categories',\n",
      "       'citation'],\n",
      "      dtype='object')\n",
      "['Donald J. Trump (2nd Term)' 'Gavin Newsom' 'Joseph R. Biden, Jr.'\n",
      " 'Donald J. Trump (1st Term)' 'Barack Obama' 'George W. Bush'\n",
      " 'William J. Clinton' 'George Bush' 'Ronald Reagan' 'Jimmy Carter'\n",
      " 'Gerald R. Ford' 'Richard Nixon' 'Lyndon B. Johnson' 'John F. Kennedy'\n",
      " 'Dwight D. Eisenhower' 'Harry S Truman' 'Franklin D. Roosevelt'\n",
      " 'Herbert Hoover' 'Calvin Coolidge' 'Warren G. Harding' 'Woodrow Wilson'\n",
      " 'William Howard Taft' 'Theodore Roosevelt' 'William McKinley'\n",
      " 'Grover Cleveland']\n",
      "25\n",
      "46\n",
      "                        president       party\n",
      "1890                 Barack Obama    Democrat\n",
      "12383             Calvin Coolidge  Republican\n",
      "1497   Donald J. Trump (1st Term)  Republican\n",
      "0      Donald J. Trump (2nd Term)  Republican\n",
      "10773        Dwight D. Eisenhower  Republican\n",
      "11805       Franklin D. Roosevelt    Democrat\n",
      "40                   Gavin Newsom         NaN\n",
      "6601                  George Bush  Republican\n",
      "3321               George W. Bush  Republican\n",
      "8533               Gerald R. Ford  Republican\n",
      "12398            Grover Cleveland    Democrat\n",
      "11336              Harry S Truman    Democrat\n",
      "12131              Herbert Hoover  Republican\n",
      "7923                 Jimmy Carter    Democrat\n",
      "10446             John F. Kennedy    Democrat\n",
      "72           Joseph R. Biden, Jr.    Democrat\n",
      "9557            Lyndon B. Johnson    Democrat\n",
      "8887                Richard Nixon  Republican\n",
      "7066                Ronald Reagan  Republican\n",
      "12396          Theodore Roosevelt  Republican\n",
      "12389           Warren G. Harding  Republican\n",
      "12395         William Howard Taft  Republican\n",
      "4491           William J. Clinton    Democrat\n",
      "12397            William McKinley  Republican\n",
      "12390              Woodrow Wilson    Democrat\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Add party affiliation based on president name\n",
    "#Make copy of df to keep original intact\n",
    "dataset_raw = dataset.copy()\n",
    "\n",
    "'''From this point on we will manipulate dataset and keep dataset_raw as original'''\n",
    "\n",
    "\n",
    "#Check dataset again\n",
    "print(dataset.columns)\n",
    "\n",
    "#Check presidents in dataset\n",
    "print(dataset['president'].unique())\n",
    "print(len(dataset['president'].unique()))\n",
    "\n",
    "#Add party affiliation\n",
    "party_affiliation = { \"Donald J. Trump (1st Term)\" : \"Republican\",\n",
    "                        \"Donald J. Trump (2nd Term)\" : \"Republican\",\n",
    "                        \"Joseph R. Biden, Jr.\": \"Democrat\",\n",
    "                        \"Barack Obama\": \"Democrat\",\n",
    "                        \"George W. Bush\": \"Republican\",\n",
    "                        \"William J. Clinton\": \"Democrat\",\n",
    "                        \"George Bush\": \"Republican\",\n",
    "                        \"Ronald Reagan\": \"Republican\",\n",
    "                        \"Jimmy Carter\": \"Democrat\",\n",
    "                        \"Gerald R. Ford\": \"Republican\",\n",
    "                        \"Richard Nixon\": \"Republican\",\n",
    "                        \"Lyndon B. Johnson\": \"Democrat\",\n",
    "                        \"John F. Kennedy\": \"Democrat\",\n",
    "                        \"Dwight D. Eisenhower\": \"Republican\",\n",
    "                        \"Harry S Truman\": \"Democrat\",\n",
    "                        \"Franklin D. Roosevelt\": \"Democrat\",\n",
    "                        \"Herbert Hoover\": \"Republican\", \n",
    "                        \"Calvin Coolidge\": \"Republican\",\n",
    "                        \"Warren G. Harding\": \"Republican\",\n",
    "                        \"Woodrow Wilson\": \"Democrat\",\n",
    "                        \"William Howard Taft\": \"Republican\",\n",
    "                        \"Theodore Roosevelt\": \"Republican\",\n",
    "                        \"William McKinley\": \"Republican\",\n",
    "                        \"Grover Cleveland\": \"Democrat\",\n",
    "                        \"Benjamin Harrison\": \"Republican\",\n",
    "                        \"Chester A. Arthur\": \"Republican\",\n",
    "                        \"James A. Garfield\": \"Republican\",\n",
    "                        \"Rutherford B. Hayes\": \"Republican\",\n",
    "                        \"Ulysses S. Grant\": \"Republican\",\n",
    "                        \"Andrew Johnson\": \"Democrat\",\n",
    "                        \"Abraham Lincoln\": \"Republican\",\n",
    "                        \"James Buchanan\": \"Democrat\",\n",
    "                        \"Franklin Pierce\": \"Democrat\",\n",
    "                        \"Millard Fillmore\": \"Whig\",\n",
    "                        \"Zachary Taylor\": \"Whig\",\n",
    "                        \"James K. Polk\": \"Democrat\",\n",
    "                        \"John Tyler\": \"Whig\",\n",
    "                        \"William Harrison\": \"Whig\",\n",
    "                        \"Martin Van Buren\": \"Democrat\",\n",
    "                        \"Andrew Jackson\": \"Democrat\",\n",
    "                        \"John Quincy Adams\": \"National Republican\",\n",
    "                        \"James Monroe\": \"Democrat-Republican\",\n",
    "                        \"James Madison\": \"Democrat-Republican\",\n",
    "                        \"Thomas Jefferson\": \"Democrat-Republican\",\n",
    "                        \"John Adams\": \"Federalist\",\n",
    "                        \"George Washington\": \"Federalist\"\n",
    "                        }\n",
    "\n",
    "#Check if we have all presidents listed\n",
    "print(len(party_affiliation))\n",
    "\n",
    "#Map party affiliation to df\n",
    "dataset['party'] = dataset['president'].map(party_affiliation)\n",
    "\n",
    "#Check if it correctly mapped\n",
    "print(dataset[['president', 'party']].drop_duplicates().sort_values(by='president'))\n",
    "\n",
    "'''Prior to the two party system we know, there were other parties such as Whig, Federalist, National Republican, and Democrat-Republican. We will keep these as is for now.'''\n",
    "\n",
    "#Remove gavin newsom speeches if any since he is not a president\n",
    "dataset = dataset[dataset['president'] != 'Gavin Newsom']\n",
    "\n",
    "#Check dataset again to see if drop worked\n",
    "\"Gavin Newsom\" in dataset['president'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75882397",
   "metadata": {},
   "source": [
    "### Dataset is inspected, now we can preprocess the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a8c58d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shime\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shime\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shime\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             content  \\\n",
      "0  On 23 November 2025, representatives of the Un...   \n",
      "1  Jeffrey Epstein, who was charged by the Trump ...   \n",
      "2  Today, the United States of America (United St...   \n",
      "3  The United States of America (United States, o...   \n",
      "4  President Donald J. Trump and President Daniel...   \n",
      "\n",
      "                                     cleaned_content  \n",
      "0  november representative united state ukraine m...  \n",
      "1  jeffrey epstein charged trump justice departme...  \n",
      "2  today united state america united state swiss ...  \n",
      "3  united state america united state u republic e...  \n",
      "4  president donald j trump president daniel nobo...  \n"
     ]
    }
   ],
   "source": [
    "#Preprocess text\n",
    "\n",
    "#Import necessary libraries\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords') # Download if needed\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt') #Download if needed\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet') #Download if needed\n",
    "\n",
    "#Define stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#Create function to preprocess text\n",
    "def preprocess(text): \n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'&[a-z]+;', ' ', text)  # Remove HTML entities\n",
    "    text = re.sub(r\"[^a-z\\s']\", ' ', text)  # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+', ' ',text).strip()  # Remove extra whitespace\n",
    "    text = re.sub (r'\\d+', '', text)  # Remove numbers\n",
    "    tokens = word_tokenize(text)  # Tokenization\n",
    "    tokens = [word for word in tokens if word not in stopwords]  # Remove stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n",
    "    return ' '.join(tokens)  # Join tokens back to string\n",
    "\n",
    "#Apply to text \n",
    "dataset['cleaned_content'] = dataset['content'].apply(preprocess)\n",
    "\n",
    "#Check df \n",
    "print(dataset[['content', 'cleaned_content']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb047cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "soviet state agreement country energy united world united state arm security\n",
      "Topic 1:\n",
      "including support country state global united security cooperation development international\n",
      "Topic 2:\n",
      "act provision section executive congress law authority national president branch\n",
      "Topic 3:\n",
      "american year today job business family america economy care work\n",
      "Topic 4:\n",
      "people american life world family nation year day state united\n",
      "Topic 5:\n",
      "president united state united state minister prime prime minister country cooperation security\n",
      "Topic 6:\n",
      "government year state congress federal people program new american work\n",
      "Topic 7:\n",
      "program health act american congress child law legislation today service\n",
      "Topic 8:\n",
      "united international state ukraine united state people support russia right country\n",
      "Topic 9:\n",
      "state united united state nuclear government weapon nation international president country\n",
      "21783.700169002244\n",
      "-37146336.044040665\n"
     ]
    }
   ],
   "source": [
    "# Topic modeling with LDA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "#Make new stopword list due to overlap in previous runs\n",
    "stopwords = ['american', 'america', 'states', 'state', 'president']\n",
    "\n",
    "#Vectorize statements \n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words=stopwords, ngram_range=(1,3))\n",
    "X = vectorizer.fit_transform(dataset['cleaned_content'])\n",
    "\n",
    "#Extract the topics \n",
    "lda = LatentDirichletAllocation(n_components=7, random_state=42)\n",
    "lda.fit(X)\n",
    "\n",
    "#Function for displaying the topics\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\"|\".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "#Display topics\n",
    "no_top_words = 10\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(lda, feature_names, no_top_words)\n",
    "\n",
    "#Topic coherence evaluation\n",
    "print(lda.perplexity(X))\n",
    "print(lda.score(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e80c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempting topic modeling with Gensim LDA\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "#Prepare data for Gensim\n",
    "texts = [doc.split() for doc in dataset['cleaned_content']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "#Build LDA model\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=15, random_state=42)\n",
    "\n",
    "#Display topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "#Evaluate topic coherence\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f\"Coherence Score: {coherence_lda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db4badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic modeling again this time with NMF\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Vectorize again using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "X_tfidf = vectorizer.fit_transform(dataset['cleaned_content'])\n",
    "\n",
    "#Fit NMF\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(X_tfidf)\n",
    "\n",
    "#Display topics\n",
    "no_top_words = 10 \n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "display_topics(nmf, feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcbaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af5774c",
   "metadata": {},
   "source": [
    "### First classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7953f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
